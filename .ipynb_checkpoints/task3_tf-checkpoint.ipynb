{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gregoire/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train = pd.read_hdf(\"train.h5\", \"train\")\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "Train=train.as_matrix()\n",
    "y_train=Train[:,0]\n",
    "x_train=Train[:,1:]\n",
    "y_train = to_categorical(y_train,5)\n",
    "Test=test.as_matrix()\n",
    "\n",
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "#define a validation set\n",
    "split_size = int(x_train.shape[0]*0.9)\n",
    "\n",
    "train_x, val_x = x_train[:split_size], x_train[split_size:]\n",
    "train_y, val_y = y_train[:split_size], y_train[split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    \n",
    "    batch_x = eval(dataset_name + '_x')[[batch_mask]].reshape(-1, input_num_units)\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        batch_y = eval(dataset_name + '_y')[[batch_mask]]\n",
    "#         batch_y = to_categorical(batch_y, num_classes=5)\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_num_units = 100 #nb of dimensions of the data\n",
    "hidden_num_units = 100\n",
    "hidden_num_units2 = 100\n",
    "output_num_units = 5 #nb of classes\n",
    "\n",
    "training_epochs = 100\n",
    "batch_size = 356\n",
    "learning_rate = 0.001\n",
    "# mom = 0.9\n",
    "\n",
    "#defining the TF graph\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  \n",
    "\n",
    "    #define placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "    y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "    # set remaining variables\n",
    "    training_epochs = 100\n",
    "    batch_size = 356\n",
    "    learning_rate = 0.001\n",
    "    mom = 0.9\n",
    "\n",
    "    ### define weights and biases of the neural network (refer this article if you don't understand the terminologies)\n",
    "\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "        'h2': tf.Variable(tf.random_normal([hidden_num_units, hidden_num_units2], seed=seed)),\n",
    "#         'h3': tf.Variable(tf.random_normal([hidden_num_units2, hidden_num_units3], seed=seed)),\n",
    "        'out': tf.Variable(tf.random_normal([hidden_num_units2, output_num_units], seed=seed))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'h1': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "        'h2': tf.Variable(tf.random_normal([hidden_num_units2], seed=seed)),\n",
    "#         'h3': tf.Variable(tf.random_normal([hidden_num_units3], seed=seed)),\n",
    "        'out': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "    }\n",
    "\n",
    "    keep_prob = tf.placeholder(\"float\")\n",
    "    #first hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['h1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    #second hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['h2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "    #output\n",
    "    logits = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    logits = tf.nn.softmax(logits)\n",
    "    \n",
    "    #loss definition\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=predictions))\n",
    "    \n",
    "    #define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'cost=', '7577.793820359')\n",
      "('Epoch:', '0002', 'cost=', '2120.100063054')\n",
      "('Epoch:', '0003', 'cost=', '947.226383720')\n",
      "('Epoch:', '0004', 'cost=', '445.492542177')\n",
      "('Epoch:', '0005', 'cost=', '154.748267947')\n",
      "('Epoch:', '0006', 'cost=', '34.974136202')\n",
      "('Epoch:', '0007', 'cost=', '14.066271483')\n",
      "('Epoch:', '0008', 'cost=', '8.694119437')\n",
      "('Epoch:', '0009', 'cost=', '6.025637871')\n",
      "('Epoch:', '0010', 'cost=', '4.643083295')\n",
      "('Epoch:', '0011', 'cost=', '3.874922632')\n",
      "('Epoch:', '0012', 'cost=', '3.292789577')\n",
      "('Epoch:', '0013', 'cost=', '3.171279842')\n",
      "('Epoch:', '0014', 'cost=', '2.815515224')\n",
      "('Epoch:', '0015', 'cost=', '2.495231736')\n",
      "('Epoch:', '0016', 'cost=', '2.346277167')\n",
      "('Epoch:', '0017', 'cost=', '2.253462567')\n",
      "('Epoch:', '0018', 'cost=', '2.137342049')\n",
      "('Epoch:', '0019', 'cost=', '1.969082993')\n",
      "('Epoch:', '0020', 'cost=', '2.001377885')\n",
      "('Epoch:', '0021', 'cost=', '1.997638307')\n",
      "('Epoch:', '0022', 'cost=', '1.865759751')\n",
      "('Epoch:', '0023', 'cost=', '1.924790274')\n",
      "('Epoch:', '0024', 'cost=', '1.880052368')\n",
      "('Epoch:', '0025', 'cost=', '1.783346953')\n",
      "('Epoch:', '0026', 'cost=', '1.776800959')\n",
      "('Epoch:', '0027', 'cost=', '1.867767982')\n",
      "('Epoch:', '0028', 'cost=', '1.737861854')\n",
      "('Epoch:', '0029', 'cost=', '1.785726900')\n",
      "('Epoch:', '0030', 'cost=', '1.679732510')\n",
      "('Epoch:', '0031', 'cost=', '1.650327834')\n",
      "('Epoch:', '0032', 'cost=', '1.625506705')\n",
      "('Epoch:', '0033', 'cost=', '1.602823306')\n",
      "('Epoch:', '0034', 'cost=', '1.630975014')\n",
      "('Epoch:', '0035', 'cost=', '1.620346556')\n",
      "('Epoch:', '0036', 'cost=', '1.585615044')\n",
      "('Epoch:', '0037', 'cost=', '1.588298760')\n",
      "('Epoch:', '0038', 'cost=', '1.600371102')\n",
      "('Epoch:', '0039', 'cost=', '1.622964230')\n",
      "('Epoch:', '0040', 'cost=', '1.545064555')\n",
      "('Epoch:', '0041', 'cost=', '1.585095924')\n",
      "('Epoch:', '0042', 'cost=', '1.574692577')\n",
      "('Epoch:', '0043', 'cost=', '1.560229990')\n",
      "('Epoch:', '0044', 'cost=', '1.552876178')\n",
      "('Epoch:', '0045', 'cost=', '1.562644948')\n",
      "('Epoch:', '0046', 'cost=', '1.565055408')\n",
      "('Epoch:', '0047', 'cost=', '1.560764587')\n",
      "('Epoch:', '0048', 'cost=', '1.540573122')\n",
      "('Epoch:', '0049', 'cost=', '1.530582838')\n",
      "('Epoch:', '0050', 'cost=', '1.529627867')\n",
      "('Epoch:', '0051', 'cost=', '1.526025447')\n",
      "('Epoch:', '0052', 'cost=', '1.570221478')\n",
      "('Epoch:', '0053', 'cost=', '1.519192072')\n",
      "('Epoch:', '0054', 'cost=', '1.511007096')\n",
      "('Epoch:', '0055', 'cost=', '1.539574910')\n",
      "('Epoch:', '0056', 'cost=', '1.546699065')\n",
      "('Epoch:', '0057', 'cost=', '1.524598150')\n",
      "('Epoch:', '0058', 'cost=', '1.527745117')\n",
      "('Epoch:', '0059', 'cost=', '1.539864071')\n",
      "('Epoch:', '0060', 'cost=', '1.522173662')\n",
      "('Epoch:', '0061', 'cost=', '1.514494223')\n",
      "('Epoch:', '0062', 'cost=', '1.551116712')\n",
      "('Epoch:', '0063', 'cost=', '1.516385092')\n",
      "('Epoch:', '0064', 'cost=', '1.517408646')\n",
      "('Epoch:', '0065', 'cost=', '1.514908367')\n",
      "('Epoch:', '0066', 'cost=', '1.510655122')\n",
      "('Epoch:', '0067', 'cost=', '1.527851450')\n",
      "('Epoch:', '0068', 'cost=', '1.544468000')\n",
      "('Epoch:', '0069', 'cost=', '1.504865051')\n",
      "('Epoch:', '0070', 'cost=', '1.531926102')\n",
      "('Epoch:', '0071', 'cost=', '1.544642496')\n",
      "('Epoch:', '0072', 'cost=', '1.523334394')\n",
      "('Epoch:', '0073', 'cost=', '1.512639569')\n",
      "('Epoch:', '0074', 'cost=', '1.541632578')\n",
      "('Epoch:', '0075', 'cost=', '1.520658328')\n",
      "('Epoch:', '0076', 'cost=', '1.515774056')\n",
      "('Epoch:', '0077', 'cost=', '1.521011834')\n",
      "('Epoch:', '0078', 'cost=', '1.546170089')\n",
      "('Epoch:', '0079', 'cost=', '1.513695287')\n",
      "('Epoch:', '0080', 'cost=', '1.515314766')\n",
      "('Epoch:', '0081', 'cost=', '1.530619833')\n",
      "('Epoch:', '0082', 'cost=', '1.536595080')\n",
      "('Epoch:', '0083', 'cost=', '1.505157953')\n",
      "('Epoch:', '0084', 'cost=', '1.510374755')\n",
      "('Epoch:', '0085', 'cost=', '1.505400048')\n",
      "('Epoch:', '0086', 'cost=', '1.503199546')\n",
      "('Epoch:', '0087', 'cost=', '1.544818413')\n",
      "('Epoch:', '0088', 'cost=', '1.506709127')\n",
      "('Epoch:', '0089', 'cost=', '1.520458855')\n",
      "('Epoch:', '0090', 'cost=', '1.505168783')\n",
      "('Epoch:', '0091', 'cost=', '1.513258790')\n",
      "('Epoch:', '0092', 'cost=', '1.525617656')\n",
      "('Epoch:', '0093', 'cost=', '1.524008899')\n",
      "('Epoch:', '0094', 'cost=', '1.503750467')\n",
      "('Epoch:', '0095', 'cost=', '1.525251649')\n",
      "('Epoch:', '0096', 'cost=', '1.505347265')\n",
      "('Epoch:', '0097', 'cost=', '1.511030845')\n",
      "('Epoch:', '0098', 'cost=', '1.526044648')\n",
      "('Epoch:', '0099', 'cost=', '1.554954561')\n",
      "('Epoch:', '0100', 'cost=', '1.521248846')\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-56ebff17ac35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mcorrect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: 0.9\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: val_x, y: val_y, keep_prob: 1.0}))\n",
    "    print(x.shape,y.shape)\n",
    "    res=sess.run(predictions,{x: test, keep_prob: 1.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the results\n",
    "tmp = np.argmax(res,axis=1)\n",
    "dff = pd.DataFrame(tmp)\n",
    "dff.reset_index(inplace=True)\n",
    "dff.columns = ['Id','y']\n",
    "dff['Id']+=45324\n",
    "# dff.columns = ['Id','y']\n",
    "#dff.to_csv(\"outputtask3.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((45324, 101), (45324, 101))\n",
      "Train on 40791 samples, validate on 4533 samples\n",
      "Epoch 1/150\n",
      "40791/40791 [==============================] - 3s 78us/step - loss: 1.0738 - acc: 0.5698 - val_loss: 0.8553 - val_acc: 0.6755\n",
      "Epoch 2/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.8425 - acc: 0.6759 - val_loss: 0.7563 - val_acc: 0.7112\n",
      "Epoch 3/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.7884 - acc: 0.6977 - val_loss: 0.7476 - val_acc: 0.7214\n",
      "Epoch 4/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.7640 - acc: 0.7056 - val_loss: 0.7090 - val_acc: 0.7278\n",
      "Epoch 5/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.7458 - acc: 0.7140 - val_loss: 0.7998 - val_acc: 0.6883\n",
      "Epoch 6/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.7332 - acc: 0.7181 - val_loss: 0.7101 - val_acc: 0.7328\n",
      "Epoch 7/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.7136 - acc: 0.7272 - val_loss: 0.7253 - val_acc: 0.7293\n",
      "Epoch 8/150\n",
      "40791/40791 [==============================] - 2s 54us/step - loss: 0.7157 - acc: 0.7265 - val_loss: 0.6779 - val_acc: 0.7467\n",
      "Epoch 9/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.6885 - acc: 0.7369 - val_loss: 0.6406 - val_acc: 0.7573\n",
      "Epoch 10/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.6772 - acc: 0.7422 - val_loss: 0.6901 - val_acc: 0.7351\n",
      "Epoch 11/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.6707 - acc: 0.7445 - val_loss: 0.6757 - val_acc: 0.7525\n",
      "Epoch 12/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.6564 - acc: 0.7518 - val_loss: 0.7062 - val_acc: 0.7386\n",
      "Epoch 13/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.6364 - acc: 0.7587 - val_loss: 0.5924 - val_acc: 0.7767\n",
      "Epoch 14/150\n",
      "40791/40791 [==============================] - 2s 51us/step - loss: 0.6253 - acc: 0.7638 - val_loss: 0.5800 - val_acc: 0.7915\n",
      "Epoch 15/150\n",
      "40791/40791 [==============================] - 2s 57us/step - loss: 0.6182 - acc: 0.7674 - val_loss: 0.5712 - val_acc: 0.7891\n",
      "Epoch 16/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.6078 - acc: 0.7736 - val_loss: 0.5709 - val_acc: 0.7915\n",
      "Epoch 17/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.5883 - acc: 0.7784 - val_loss: 0.5530 - val_acc: 0.7973\n",
      "Epoch 18/150\n",
      "40791/40791 [==============================] - 2s 51us/step - loss: 0.5875 - acc: 0.7798 - val_loss: 0.6138 - val_acc: 0.7728\n",
      "Epoch 19/150\n",
      "40791/40791 [==============================] - 3s 65us/step - loss: 0.5705 - acc: 0.7881 - val_loss: 0.5122 - val_acc: 0.8143\n",
      "Epoch 20/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.5580 - acc: 0.7916 - val_loss: 0.5056 - val_acc: 0.8195\n",
      "Epoch 21/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.5498 - acc: 0.7939 - val_loss: 0.5585 - val_acc: 0.7944\n",
      "Epoch 22/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.5363 - acc: 0.8011 - val_loss: 0.4768 - val_acc: 0.8321\n",
      "Epoch 23/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.5352 - acc: 0.8023 - val_loss: 0.5256 - val_acc: 0.8087\n",
      "Epoch 24/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.5265 - acc: 0.8059 - val_loss: 0.4792 - val_acc: 0.8301\n",
      "Epoch 25/150\n",
      "40791/40791 [==============================] - 2s 56us/step - loss: 0.5132 - acc: 0.8086 - val_loss: 0.5475 - val_acc: 0.7999\n",
      "Epoch 26/150\n",
      "40791/40791 [==============================] - 3s 63us/step - loss: 0.5063 - acc: 0.8111 - val_loss: 0.4503 - val_acc: 0.8354\n",
      "Epoch 27/150\n",
      "40791/40791 [==============================] - 2s 51us/step - loss: 0.4977 - acc: 0.8167 - val_loss: 0.4405 - val_acc: 0.8412\n",
      "Epoch 28/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.4897 - acc: 0.8196 - val_loss: 0.4494 - val_acc: 0.8381\n",
      "Epoch 29/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.4868 - acc: 0.8202 - val_loss: 0.4788 - val_acc: 0.8293\n",
      "Epoch 30/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.4856 - acc: 0.8193 - val_loss: 0.4702 - val_acc: 0.8281\n",
      "Epoch 31/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.4800 - acc: 0.8230 - val_loss: 0.4376 - val_acc: 0.8465\n",
      "Epoch 32/150\n",
      "40791/40791 [==============================] - 2s 54us/step - loss: 0.4750 - acc: 0.8233 - val_loss: 0.4404 - val_acc: 0.8449\n",
      "Epoch 33/150\n",
      "40791/40791 [==============================] - 3s 73us/step - loss: 0.4671 - acc: 0.8268 - val_loss: 0.4427 - val_acc: 0.8414\n",
      "Epoch 34/150\n",
      "40791/40791 [==============================] - 2s 57us/step - loss: 0.4600 - acc: 0.8291 - val_loss: 0.4332 - val_acc: 0.8460\n",
      "Epoch 35/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.4614 - acc: 0.8321 - val_loss: 0.4959 - val_acc: 0.8200\n",
      "Epoch 36/150\n",
      "40791/40791 [==============================] - 3s 63us/step - loss: 0.4532 - acc: 0.8318 - val_loss: 0.4306 - val_acc: 0.8484\n",
      "Epoch 37/150\n",
      "40791/40791 [==============================] - 2s 57us/step - loss: 0.4613 - acc: 0.8297 - val_loss: 0.4400 - val_acc: 0.8368\n",
      "Epoch 38/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.4433 - acc: 0.8349 - val_loss: 0.4011 - val_acc: 0.8586\n",
      "Epoch 39/150\n",
      "40791/40791 [==============================] - 2s 55us/step - loss: 0.4376 - acc: 0.8382 - val_loss: 0.4213 - val_acc: 0.8495\n",
      "Epoch 40/150\n",
      "40791/40791 [==============================] - 2s 54us/step - loss: 0.4346 - acc: 0.8416 - val_loss: 0.4184 - val_acc: 0.8502\n",
      "Epoch 41/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.4292 - acc: 0.8406 - val_loss: 0.4054 - val_acc: 0.8593\n",
      "Epoch 42/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.4317 - acc: 0.8417 - val_loss: 0.4284 - val_acc: 0.8484\n",
      "Epoch 43/150\n",
      "40791/40791 [==============================] - 2s 60us/step - loss: 0.4262 - acc: 0.8429 - val_loss: 0.4075 - val_acc: 0.8586\n",
      "Epoch 44/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.4182 - acc: 0.8471 - val_loss: 0.4167 - val_acc: 0.8544\n",
      "Epoch 45/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.4221 - acc: 0.8442 - val_loss: 0.3953 - val_acc: 0.8601\n",
      "Epoch 46/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.4150 - acc: 0.8481 - val_loss: 0.3982 - val_acc: 0.8588\n",
      "Epoch 47/150\n",
      "40791/40791 [==============================] - 2s 53us/step - loss: 0.4152 - acc: 0.8454 - val_loss: 0.3973 - val_acc: 0.8639\n",
      "Epoch 48/150\n",
      "40791/40791 [==============================] - 2s 58us/step - loss: 0.4180 - acc: 0.8451 - val_loss: 0.3913 - val_acc: 0.8568\n",
      "Epoch 49/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3986 - acc: 0.8533 - val_loss: 0.3798 - val_acc: 0.8663\n",
      "Epoch 50/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.4064 - acc: 0.8514 - val_loss: 0.3622 - val_acc: 0.8754\n",
      "Epoch 51/150\n",
      "40791/40791 [==============================] - 2s 55us/step - loss: 0.4027 - acc: 0.8518 - val_loss: 0.3842 - val_acc: 0.8612\n",
      "Epoch 52/150\n",
      "40791/40791 [==============================] - 2s 53us/step - loss: 0.4044 - acc: 0.8509 - val_loss: 0.4040 - val_acc: 0.8553\n",
      "Epoch 53/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3946 - acc: 0.8543 - val_loss: 0.3873 - val_acc: 0.8626\n",
      "Epoch 54/150\n",
      "40791/40791 [==============================] - ETA: 0s - loss: 0.3891 - acc: 0.858 - 2s 58us/step - loss: 0.3903 - acc: 0.8573 - val_loss: 0.3908 - val_acc: 0.8626\n",
      "Epoch 55/150\n",
      "40791/40791 [==============================] - 2s 59us/step - loss: 0.3896 - acc: 0.8557 - val_loss: 0.3578 - val_acc: 0.8698\n",
      "Epoch 56/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.3862 - acc: 0.8579 - val_loss: 0.3581 - val_acc: 0.8701\n",
      "Epoch 57/150\n",
      "40791/40791 [==============================] - 3s 62us/step - loss: 0.3827 - acc: 0.8595 - val_loss: 0.3616 - val_acc: 0.8740\n",
      "Epoch 58/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.3824 - acc: 0.8604 - val_loss: 0.3593 - val_acc: 0.8736\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40791/40791 [==============================] - 2s 44us/step - loss: 0.3867 - acc: 0.8577 - val_loss: 0.3786 - val_acc: 0.8668\n",
      "Epoch 60/150\n",
      "40791/40791 [==============================] - 2s 45us/step - loss: 0.3840 - acc: 0.8589 - val_loss: 0.3591 - val_acc: 0.8765\n",
      "Epoch 61/150\n",
      "40791/40791 [==============================] - 2s 43us/step - loss: 0.3743 - acc: 0.8625 - val_loss: 0.3627 - val_acc: 0.8740\n",
      "Epoch 62/150\n",
      "40791/40791 [==============================] - 3s 62us/step - loss: 0.3770 - acc: 0.8609 - val_loss: 0.3353 - val_acc: 0.8882\n",
      "Epoch 63/150\n",
      "40791/40791 [==============================] - 2s 53us/step - loss: 0.3778 - acc: 0.8633 - val_loss: 0.3760 - val_acc: 0.8687\n",
      "Epoch 64/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3700 - acc: 0.8647 - val_loss: 0.3579 - val_acc: 0.8751\n",
      "Epoch 65/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.3690 - acc: 0.8646 - val_loss: 0.3349 - val_acc: 0.8844\n",
      "Epoch 66/150\n",
      "40791/40791 [==============================] - 2s 53us/step - loss: 0.3729 - acc: 0.8648 - val_loss: 0.3322 - val_acc: 0.8862\n",
      "Epoch 67/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3673 - acc: 0.8652 - val_loss: 0.3432 - val_acc: 0.8837\n",
      "Epoch 68/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3638 - acc: 0.8663 - val_loss: 0.3500 - val_acc: 0.8743\n",
      "Epoch 69/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3606 - acc: 0.8687 - val_loss: 0.3477 - val_acc: 0.8745\n",
      "Epoch 70/150\n",
      "40791/40791 [==============================] - 2s 54us/step - loss: 0.3635 - acc: 0.8667 - val_loss: 0.3335 - val_acc: 0.8831\n",
      "Epoch 71/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3606 - acc: 0.8674 - val_loss: 0.3389 - val_acc: 0.8824\n",
      "Epoch 72/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3617 - acc: 0.8670 - val_loss: 0.3397 - val_acc: 0.8776\n",
      "Epoch 73/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3552 - acc: 0.8709 - val_loss: 0.3327 - val_acc: 0.8840\n",
      "Epoch 74/150\n",
      "40791/40791 [==============================] - 2s 45us/step - loss: 0.3566 - acc: 0.8697 - val_loss: 0.3367 - val_acc: 0.8831\n",
      "Epoch 75/150\n",
      "40791/40791 [==============================] - 2s 54us/step - loss: 0.3576 - acc: 0.8703 - val_loss: 0.3156 - val_acc: 0.8906\n",
      "Epoch 76/150\n",
      "40791/40791 [==============================] - 2s 51us/step - loss: 0.3497 - acc: 0.8734 - val_loss: 0.3493 - val_acc: 0.8745\n",
      "Epoch 77/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3512 - acc: 0.8737 - val_loss: 0.3201 - val_acc: 0.8851\n",
      "Epoch 78/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3473 - acc: 0.8741 - val_loss: 0.3191 - val_acc: 0.8886\n",
      "Epoch 79/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3505 - acc: 0.8730 - val_loss: 0.3459 - val_acc: 0.8789\n",
      "Epoch 80/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3464 - acc: 0.8736 - val_loss: 0.3430 - val_acc: 0.8765\n",
      "Epoch 81/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3433 - acc: 0.8758 - val_loss: 0.3319 - val_acc: 0.8789\n",
      "Epoch 82/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3423 - acc: 0.8745 - val_loss: 0.3402 - val_acc: 0.8840\n",
      "Epoch 83/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3429 - acc: 0.8761 - val_loss: 0.3935 - val_acc: 0.8610\n",
      "Epoch 84/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3434 - acc: 0.8766 - val_loss: 0.3328 - val_acc: 0.8811\n",
      "Epoch 85/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3409 - acc: 0.8754 - val_loss: 0.3148 - val_acc: 0.8897\n",
      "Epoch 86/150\n",
      "40791/40791 [==============================] - 3s 62us/step - loss: 0.3404 - acc: 0.8764 - val_loss: 0.3161 - val_acc: 0.8899\n",
      "Epoch 87/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3403 - acc: 0.8776 - val_loss: 0.3201 - val_acc: 0.8833\n",
      "Epoch 88/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3348 - acc: 0.8790 - val_loss: 0.3266 - val_acc: 0.8868: 0s - loss: 0.3\n",
      "Epoch 89/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.3366 - acc: 0.8788 - val_loss: 0.3244 - val_acc: 0.8904\n",
      "Epoch 90/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.3349 - acc: 0.8775 - val_loss: 0.3210 - val_acc: 0.8848\n",
      "Epoch 91/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.3384 - acc: 0.8760 - val_loss: 0.3244 - val_acc: 0.8831\n",
      "Epoch 92/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3310 - acc: 0.8793 - val_loss: 0.3201 - val_acc: 0.8833\n",
      "Epoch 93/150\n",
      "40791/40791 [==============================] - 2s 54us/step - loss: 0.3341 - acc: 0.8785 - val_loss: 0.3134 - val_acc: 0.8937\n",
      "Epoch 94/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3301 - acc: 0.8794 - val_loss: 0.3269 - val_acc: 0.8879\n",
      "Epoch 95/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3292 - acc: 0.8811 - val_loss: 0.3422 - val_acc: 0.8807\n",
      "Epoch 96/150\n",
      "40791/40791 [==============================] - 2s 45us/step - loss: 0.3306 - acc: 0.8799 - val_loss: 0.3154 - val_acc: 0.8930\n",
      "Epoch 97/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3294 - acc: 0.8811 - val_loss: 0.3164 - val_acc: 0.8877\n",
      "Epoch 98/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.3294 - acc: 0.8802 - val_loss: 0.3170 - val_acc: 0.8857\n",
      "Epoch 99/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3301 - acc: 0.8792 - val_loss: 0.3269 - val_acc: 0.8820\n",
      "Epoch 100/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3273 - acc: 0.8823 - val_loss: 0.3358 - val_acc: 0.8859\n",
      "Epoch 101/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3205 - acc: 0.8832 - val_loss: 0.3057 - val_acc: 0.8915\n",
      "Epoch 102/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3266 - acc: 0.8827 - val_loss: 0.3082 - val_acc: 0.8948\n",
      "Epoch 103/150\n",
      "40791/40791 [==============================] - 2s 53us/step - loss: 0.3201 - acc: 0.8839 - val_loss: 0.3113 - val_acc: 0.8959\n",
      "Epoch 104/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.3213 - acc: 0.8839 - val_loss: 0.3271 - val_acc: 0.8862\n",
      "Epoch 105/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3230 - acc: 0.8824 - val_loss: 0.3104 - val_acc: 0.8886\n",
      "Epoch 106/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3191 - acc: 0.8845 - val_loss: 0.3106 - val_acc: 0.8921\n",
      "Epoch 107/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3171 - acc: 0.8851 - val_loss: 0.3122 - val_acc: 0.8893\n",
      "Epoch 108/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3176 - acc: 0.8854 - val_loss: 0.3191 - val_acc: 0.8890\n",
      "Epoch 109/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3240 - acc: 0.8834 - val_loss: 0.3280 - val_acc: 0.8829\n",
      "Epoch 110/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.3190 - acc: 0.8829 - val_loss: 0.3030 - val_acc: 0.8996\n",
      "Epoch 111/150\n",
      "40791/40791 [==============================] - 2s 54us/step - loss: 0.3112 - acc: 0.8866 - val_loss: 0.3347 - val_acc: 0.8829\n",
      "Epoch 112/150\n",
      "40791/40791 [==============================] - 2s 57us/step - loss: 0.3163 - acc: 0.8841 - val_loss: 0.3352 - val_acc: 0.8893\n",
      "Epoch 113/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3137 - acc: 0.8860 - val_loss: 0.3060 - val_acc: 0.8932\n",
      "Epoch 114/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3142 - acc: 0.8855 - val_loss: 0.3539 - val_acc: 0.8736\n",
      "Epoch 115/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.3127 - acc: 0.8867 - val_loss: 0.3085 - val_acc: 0.8910\n",
      "Epoch 116/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3096 - acc: 0.8878 - val_loss: 0.3152 - val_acc: 0.8866\n",
      "Epoch 117/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3061 - acc: 0.8895 - val_loss: 0.3181 - val_acc: 0.8888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.3159 - acc: 0.8865 - val_loss: 0.2960 - val_acc: 0.8965\n",
      "Epoch 119/150\n",
      "40791/40791 [==============================] - 2s 60us/step - loss: 0.3101 - acc: 0.8890 - val_loss: 0.2970 - val_acc: 0.8992\n",
      "Epoch 120/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3082 - acc: 0.8881 - val_loss: 0.3079 - val_acc: 0.8957\n",
      "Epoch 121/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3095 - acc: 0.8890 - val_loss: 0.3132 - val_acc: 0.8941\n",
      "Epoch 122/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3059 - acc: 0.8895 - val_loss: 0.3301 - val_acc: 0.8818\n",
      "Epoch 123/150\n",
      "40791/40791 [==============================] - 2s 44us/step - loss: 0.3013 - acc: 0.8915 - val_loss: 0.3079 - val_acc: 0.8930\n",
      "Epoch 124/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3116 - acc: 0.8886 - val_loss: 0.3032 - val_acc: 0.8901\n",
      "Epoch 125/150\n",
      "40791/40791 [==============================] - 2s 50us/step - loss: 0.3041 - acc: 0.8898 - val_loss: 0.3144 - val_acc: 0.8908\n",
      "Epoch 126/150\n",
      "40791/40791 [==============================] - 2s 44us/step - loss: 0.3085 - acc: 0.8878 - val_loss: 0.2932 - val_acc: 0.8983\n",
      "Epoch 127/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3031 - acc: 0.8898 - val_loss: 0.3206 - val_acc: 0.8884\n",
      "Epoch 128/150\n",
      "40791/40791 [==============================] - 2s 54us/step - loss: 0.3025 - acc: 0.8905 - val_loss: 0.3186 - val_acc: 0.8921\n",
      "Epoch 129/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.3083 - acc: 0.8879 - val_loss: 0.3197 - val_acc: 0.8886\n",
      "Epoch 130/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3076 - acc: 0.8880 - val_loss: 0.3050 - val_acc: 0.8959\n",
      "Epoch 131/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3030 - acc: 0.8906 - val_loss: 0.3054 - val_acc: 0.8939\n",
      "Epoch 132/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3019 - acc: 0.8918 - val_loss: 0.2957 - val_acc: 0.8943\n",
      "Epoch 133/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.2987 - acc: 0.8921 - val_loss: 0.3213 - val_acc: 0.8877\n",
      "Epoch 134/150\n",
      "40791/40791 [==============================] - 2s 51us/step - loss: 0.3014 - acc: 0.8904 - val_loss: 0.3028 - val_acc: 0.8893\n",
      "Epoch 135/150\n",
      "40791/40791 [==============================] - 2s 52us/step - loss: 0.2952 - acc: 0.8933 - val_loss: 0.2878 - val_acc: 0.9029\n",
      "Epoch 136/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3033 - acc: 0.8891 - val_loss: 0.3063 - val_acc: 0.8941\n",
      "Epoch 137/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.3016 - acc: 0.8913 - val_loss: 0.2994 - val_acc: 0.8954\n",
      "Epoch 138/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.3015 - acc: 0.8893 - val_loss: 0.3148 - val_acc: 0.8948\n",
      "Epoch 139/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.2976 - acc: 0.8928 - val_loss: 0.2881 - val_acc: 0.9009\n",
      "Epoch 140/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.2990 - acc: 0.8911 - val_loss: 0.3045 - val_acc: 0.8941\n",
      "Epoch 141/150\n",
      "40791/40791 [==============================] - 2s 49us/step - loss: 0.2945 - acc: 0.8946 - val_loss: 0.3528 - val_acc: 0.8751\n",
      "Epoch 142/150\n",
      "40791/40791 [==============================] - 2s 46us/step - loss: 0.3019 - acc: 0.8904 - val_loss: 0.3071 - val_acc: 0.8926\n",
      "Epoch 143/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.2997 - acc: 0.8914 - val_loss: 0.3010 - val_acc: 0.8950\n",
      "Epoch 144/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.2973 - acc: 0.8928 - val_loss: 0.3154 - val_acc: 0.8884\n",
      "Epoch 145/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.2940 - acc: 0.8934 - val_loss: 0.2938 - val_acc: 0.8983\n",
      "Epoch 146/150\n",
      "40791/40791 [==============================] - 2s 51us/step - loss: 0.2954 - acc: 0.8946 - val_loss: 0.2987 - val_acc: 0.9009\n",
      "Epoch 147/150\n",
      "40791/40791 [==============================] - 2s 55us/step - loss: 0.2905 - acc: 0.8956 - val_loss: 0.3060 - val_acc: 0.8943\n",
      "Epoch 148/150\n",
      "40791/40791 [==============================] - 2s 48us/step - loss: 0.2974 - acc: 0.8928 - val_loss: 0.3011 - val_acc: 0.8943\n",
      "Epoch 149/150\n",
      "40791/40791 [==============================] - 2s 47us/step - loss: 0.2937 - acc: 0.8938 - val_loss: 0.2952 - val_acc: 0.8976\n",
      "Epoch 150/150\n",
      "40791/40791 [==============================] - 2s 44us/step - loss: 0.2940 - acc: 0.8951 - val_loss: 0.2968 - val_acc: 0.8950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ee62910>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% KERAS\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Conv1D\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "#%%\n",
    "\n",
    "DropoutRate = 0.05\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(100, input_shape=(100,)),\n",
    "    Activation('tanh'),\n",
    "    Dropout(DropoutRate),\n",
    "    Dense(100),\n",
    "    Activation('tanh'),\n",
    "    Dropout(DropoutRate),\n",
    "    \n",
    "#     Dense(10),\n",
    "#     Activation('relu'),\n",
    "#     Dropout(DropoutRate),\n",
    "\n",
    "#     Dense(100),\n",
    "#     Activation('relu'),\n",
    "#     Dropout(DropoutRate),\n",
    "\n",
    "#     Dense(100),\n",
    "#     Activation('relu'),\n",
    "#     Dropout(DropoutRate),\n",
    "\n",
    "#     Dense(100),\n",
    "#     Activation('relu'),\n",
    "#     Dropout(DropoutRate),\n",
    "\n",
    "    Dense(5),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "# Compile the model\n",
    "import keras.backend as K\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#%%\n",
    "# Read in the data\n",
    "train = pd.read_hdf(\"train.h5\", \"train\")\n",
    "test  = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "TrainData = train.as_matrix()\n",
    "print(train.shape,TrainData.shape)\n",
    "\n",
    "y_train=TrainData[:,0]\n",
    "x_train=TrainData[:,1:]\n",
    "#%%\n",
    "# Training\n",
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(y_train)\n",
    "\n",
    "model.fit(x_train, y_binary, epochs=150, batch_size=128, validation_split = 0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8137, 100)\n",
      "(8137,)\n"
     ]
    }
   ],
   "source": [
    "tmp=test.as_matrix()\n",
    "print(tmp.shape)\n",
    "res = model.predict_classes(tmp,batch_size=1)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 3 ... 3 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.DataFrame(res)\n",
    "dff.reset_index(inplace=True)\n",
    "dff.columns = ['Id','y']\n",
    "dff['Id']+=45324\n",
    "# dff.columns = ['Id','y']\n",
    "dff.to_csv(\"outputtask3.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
