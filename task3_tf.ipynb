{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gregoire/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train = pd.read_hdf(\"train.h5\", \"train\")\n",
    "test = pd.read_hdf(\"test.h5\", \"test\")\n",
    "\n",
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=train.as_matrix()\n",
    "y_train=Train[:,0]\n",
    "x_train=Train[:,1:]\n",
    "y_train = to_categorical(y_train,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a validation set\n",
    "split_size = int(x_train.shape[0]*0.8)\n",
    "\n",
    "train_x, val_x = x_train[:split_size], x_train[split_size:]\n",
    "train_y, val_y = y_train[:split_size], y_train[split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    \n",
    "    batch_x = eval(dataset_name + '_x')[[batch_mask]].reshape(-1, input_num_units)\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        batch_y = eval(dataset_name + '_y')[[batch_mask]]\n",
    "#         batch_y = to_categorical(batch_y, num_classes=5)\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of neurons in each layer\n",
    "input_num_units = 100 #nb of dimensions of the data\n",
    "hidden_num_units = 500\n",
    "hidden_num_units2 = 150\n",
    "hidden_num_units3 = 100\n",
    "\n",
    "output_num_units = 5 #nb of classes\n",
    "\n",
    "#define placeholders\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "# set remaining variables\n",
    "training_epochs = 100\n",
    "batch_size = 300\n",
    "learning_rate = 0.001\n",
    "mom = 0.8\n",
    "\n",
    "### define weights and biases of the neural network (refer this article if you don't understand the terminologies)\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'h2': tf.Variable(tf.random_normal([hidden_num_units, hidden_num_units2], seed=seed)),\n",
    "#     'h3': tf.Variable(tf.random_normal([hidden_num_units2, hidden_num_units3], seed=seed)),\n",
    "#     'h4': tf.Variable(tf.random_normal([hidden_num_units, hidden_num_units], seed=seed)),\n",
    "    'out': tf.Variable(tf.random_normal([hidden_num_units2, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'h1': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'h2': tf.Variable(tf.random_normal([hidden_num_units2], seed=seed)),\n",
    "#     'h3': tf.Variable(tf.random_normal([hidden_num_units3], seed=seed)),\n",
    "#     'h4': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'out': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the neural net computational graph\n",
    "def NeuralNetDeep(x, weights, biases, keep_prob):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['h1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['h2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "#     layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['h3'])\n",
    "#     layer_3 = tf.nn.relu(layer_3)\n",
    "#     layer_3 = tf.nn.dropout(layer_3, keep_prob)\n",
    "#     layer_4 = tf.add(tf.matmul(layer_3, weights['h3']), biases['h4'])\n",
    "#     layer_4 = tf.nn.relu(layer_4)\n",
    "#     layer_4 = tf.nn.dropout(layer_4, keep_prob)\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "predictions = NeuralNetDeep(x, weights, biases, keep_prob)\n",
    "#cost function\n",
    "beta=0.001\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=predictions))\n",
    "regularizer = tf.nn.l2_loss(weights['h1'])\n",
    "regularizer += tf.nn.l2_loss(weights['h2'])\n",
    "# regularizer += tf.nn.l2_loss(weights['h3'])\n",
    "regularizer += tf.nn.l2_loss(weights['out'])\n",
    "cost = tf.reduce_mean(cost + beta * regularizer)\n",
    "\n",
    "#define optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', '0001', 'cost=', '391.296293852')\n",
      "('Epoch:', '0002', 'cost=', '100.029109677')\n",
      "('Epoch:', '0003', 'cost=', '58.262847774')\n",
      "('Epoch:', '0004', 'cost=', '50.828200839')\n",
      "('Epoch:', '0005', 'cost=', '47.214035868')\n",
      "('Epoch:', '0006', 'cost=', '44.578770265')\n",
      "('Epoch:', '0007', 'cost=', '42.508264908')\n",
      "('Epoch:', '0008', 'cost=', '40.785834811')\n",
      "('Epoch:', '0009', 'cost=', '39.362629720')\n",
      "('Epoch:', '0010', 'cost=', '38.152754373')\n",
      "('Epoch:', '0011', 'cost=', '37.084663265')\n",
      "('Epoch:', '0012', 'cost=', '36.139499007')\n",
      "('Epoch:', '0013', 'cost=', '35.275886485')\n",
      "('Epoch:', '0014', 'cost=', '34.499978249')\n",
      "('Epoch:', '0015', 'cost=', '33.786148551')\n",
      "('Epoch:', '0016', 'cost=', '33.102248186')\n",
      "('Epoch:', '0017', 'cost=', '32.454754912')\n",
      "('Epoch:', '0018', 'cost=', '31.838868400')\n",
      "('Epoch:', '0019', 'cost=', '31.222747550')\n",
      "('Epoch:', '0020', 'cost=', '30.638034783')\n",
      "('Epoch:', '0021', 'cost=', '30.054750203')\n",
      "('Epoch:', '0022', 'cost=', '29.470630797')\n",
      "('Epoch:', '0023', 'cost=', '28.891211188')\n",
      "('Epoch:', '0024', 'cost=', '28.314838612')\n",
      "('Epoch:', '0025', 'cost=', '27.737022046')\n",
      "('Epoch:', '0026', 'cost=', '27.141802996')\n",
      "('Epoch:', '0027', 'cost=', '26.555838642')\n",
      "('Epoch:', '0028', 'cost=', '25.895960865')\n",
      "('Epoch:', '0029', 'cost=', '25.229459523')\n",
      "('Epoch:', '0030', 'cost=', '24.592684992')\n",
      "('Epoch:', '0031', 'cost=', '23.963075196')\n",
      "('Epoch:', '0032', 'cost=', '23.326433018')\n",
      "('Epoch:', '0033', 'cost=', '22.677895931')\n",
      "('Epoch:', '0034', 'cost=', '22.035710531')\n",
      "('Epoch:', '0035', 'cost=', '21.386193661')\n",
      "('Epoch:', '0036', 'cost=', '20.734579566')\n",
      "('Epoch:', '0037', 'cost=', '20.070954948')\n",
      "('Epoch:', '0038', 'cost=', '19.411358562')\n",
      "('Epoch:', '0039', 'cost=', '18.733427831')\n",
      "('Epoch:', '0040', 'cost=', '18.066649620')\n",
      "('Epoch:', '0041', 'cost=', '17.400268125')\n",
      "('Epoch:', '0042', 'cost=', '16.729558300')\n",
      "('Epoch:', '0043', 'cost=', '16.058342536')\n",
      "('Epoch:', '0044', 'cost=', '15.398165235')\n",
      "('Epoch:', '0045', 'cost=', '14.735807924')\n",
      "('Epoch:', '0046', 'cost=', '14.071009150')\n",
      "('Epoch:', '0047', 'cost=', '13.432069002')\n",
      "('Epoch:', '0048', 'cost=', '12.784659563')\n",
      "('Epoch:', '0049', 'cost=', '12.150904163')\n",
      "('Epoch:', '0050', 'cost=', '11.523878792')\n",
      "('Epoch:', '0051', 'cost=', '10.922511006')\n",
      "('Epoch:', '0052', 'cost=', '10.307670416')\n",
      "('Epoch:', '0053', 'cost=', '9.686646038')\n",
      "('Epoch:', '0054', 'cost=', '9.087103307')\n",
      "('Epoch:', '0055', 'cost=', '8.462978849')\n",
      "('Epoch:', '0056', 'cost=', '7.877489046')\n",
      "('Epoch:', '0057', 'cost=', '7.341677858')\n",
      "('Epoch:', '0058', 'cost=', '6.859033266')\n",
      "('Epoch:', '0059', 'cost=', '6.404021753')\n",
      "('Epoch:', '0060', 'cost=', '5.959703022')\n",
      "('Epoch:', '0061', 'cost=', '5.566686684')\n",
      "('Epoch:', '0062', 'cost=', '5.179772020')\n",
      "('Epoch:', '0063', 'cost=', '4.827549641')\n",
      "('Epoch:', '0064', 'cost=', '4.497403366')\n",
      "('Epoch:', '0065', 'cost=', '4.164212648')\n",
      "('Epoch:', '0066', 'cost=', '3.891032491')\n",
      "('Epoch:', '0067', 'cost=', '3.622630541')\n",
      "('Epoch:', '0068', 'cost=', '3.367726305')\n",
      "('Epoch:', '0069', 'cost=', '3.149865379')\n",
      "('Epoch:', '0070', 'cost=', '2.955736978')\n",
      "('Epoch:', '0071', 'cost=', '2.774141124')\n",
      "('Epoch:', '0072', 'cost=', '2.610092086')\n",
      "('Epoch:', '0073', 'cost=', '2.467992677')\n",
      "('Epoch:', '0074', 'cost=', '2.330640769')\n",
      "('Epoch:', '0075', 'cost=', '2.218290703')\n",
      "('Epoch:', '0076', 'cost=', '2.110003819')\n",
      "('Epoch:', '0077', 'cost=', '2.024559779')\n",
      "('Epoch:', '0078', 'cost=', '1.950228595')\n",
      "('Epoch:', '0079', 'cost=', '1.873734682')\n",
      "('Epoch:', '0080', 'cost=', '1.820043149')\n",
      "('Epoch:', '0081', 'cost=', '1.759881575')\n",
      "('Epoch:', '0082', 'cost=', '1.709410255')\n",
      "('Epoch:', '0083', 'cost=', '1.658989367')\n",
      "('Epoch:', '0084', 'cost=', '1.617392201')\n",
      "('Epoch:', '0085', 'cost=', '1.586174986')\n",
      "('Epoch:', '0086', 'cost=', '1.555437586')\n",
      "('Epoch:', '0087', 'cost=', '1.524811870')\n",
      "('Epoch:', '0088', 'cost=', '1.494525066')\n",
      "('Epoch:', '0089', 'cost=', '1.469943380')\n",
      "('Epoch:', '0090', 'cost=', '1.438543705')\n",
      "('Epoch:', '0091', 'cost=', '1.424535718')\n",
      "('Epoch:', '0092', 'cost=', '1.397250018')\n",
      "('Epoch:', '0093', 'cost=', '1.378684426')\n",
      "('Epoch:', '0094', 'cost=', '1.359145457')\n",
      "('Epoch:', '0095', 'cost=', '1.341294489')\n",
      "('Epoch:', '0096', 'cost=', '1.330580751')\n",
      "('Epoch:', '0097', 'cost=', '1.303784728')\n",
      "('Epoch:', '0098', 'cost=', '1.301159659')\n",
      "('Epoch:', '0099', 'cost=', '1.287693984')\n",
      "('Epoch:', '0100', 'cost=', '1.267587944')\n",
      "Optimization Finished!\n",
      "('Accuracy:', 0.8589079)\n",
      "(TensorShape([Dimension(None), Dimension(100)]), TensorShape([Dimension(None), Dimension(5)]))\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: 0.8\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: val_x, y: val_y, keep_prob: 1.0}))\n",
    "    print(x.shape,y.shape)\n",
    "    res=sess.run(predictions,{x: test, keep_prob: 1.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the results\n",
    "tmp = np.argmax(res,axis=1)\n",
    "dff = pd.DataFrame(tmp)\n",
    "dff.reset_index(inplace=True)\n",
    "dff.columns = ['Id','y']\n",
    "dff['Id']+=45324\n",
    "# dff.columns = ['Id','y']\n",
    "dff.to_csv(\"outputtask3.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
